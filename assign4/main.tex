\documentclass[12pt,a4paper]{article}

% Package imports
\usepackage[utf8]{inputenc}        % Allows UTF-8 input
\usepackage{amsmath,amsfonts,amssymb} % For math symbols
\usepackage{graphicx}               % For including graphics
\usepackage{hyperref}               % For hyperlinks
\usepackage{geometry}               % Set page dimensions
\usepackage{fancyhdr}               % For custom headers/footers
\usepackage{enumitem}               % For custom lists
\usepackage{setspace}               % For line spacing
\usepackage{titlesec}               % For section formatting
\usepackage{booktabs}               % For nicer table formatting
\usepackage{array}                  % For advanced table options
\usepackage{subcaption}  % Add this to your preamble
\usepackage[
backend=biber,
style=ieee,
]{biblatex}                         % For bibliography
\addbibresource{references.bib}     % Add bibliography file

% Set the margins (adjust as needed)
\geometry{
    top=1in,
    bottom=1in,
    left=1in,
    right=1in,
}

% Customizing the header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage}             % Centered page number

% Section title formatting
\titleformat{\section}{\large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection}{1em}{}

% Set line spacing
\setstretch{1}

% Title and author (can be customized as needed)
\title{Clustering Agorithm on Mice Protein Dataset}
\author{Leihan Chen}
\date{\today}

% Document starts here
\begin{document}

% \maketitle

% Introduction section
\section{Background}
Mice Protein Expression dataset is a multi-class classification/clusteirng dataset. 
The dataset consists of 77 proteins/protein modifications of expression level that produced detectable signals in the nuclear fraction of cortex \cite{mice_protein_expression_342}. 
Two types of mice, control (38) and trisomic (34), were used in the experiment. 15 Measurements were taken for each protein across each sample. Therefore, the dataset contains 1080 instances per protein \cite{Higuera2015SelfOrganizingFM}.
The eight classes of mice are then described based on three categorical features genotype, behavior and treatment. In summary, the dataset contains 1080 instances with 77 different protein modifications (features) with 8 types of mice.

The data is obtained from the UCI Machine Learning Repository \cite{mice_protein_expression_342}. This dataset can be applied as both a multi-class classification and clustering problem. In this report, the dataset is treated as a clustering problem. 
The goal is to cluster the mice samples based on the protein expression levels. This dataset is challenging due to the high dimensionality. 
Meanwhile, the number of instances is relatively small, which is suitable for some clustering algorithms such as Hierarical Clustering. A visualization of the dataset (featrures are reduced to 2 for visualization purpose) is shown in the subfigure of figure \ref{fig:comparison_clustering}.

% A visualization of label distribution is shown in figure \ref{fig:label_distribution}. It is clearly observed that the class is imbalanced, with the largest class being with 3546 instances while the smallest only contains 522 instances.
% Moreover, figure \ref{fig:dry_bean_violin} shows the violin plot of each feature (all features are normalized for visualization convenience). 
% The figure demonstrates that the dimensional features are approximately normally distributed while the shape features are opposite. Meanwhile, some features such as Solidity, ConvexArea, and AxisLength have some significant outliers.

% Methodology section
\section{Methods}\label{sec:methods}
\subsection{Pre-processing}\label{subsec:preprocessing}
It is noticed that the dataset has missing values in many features. Therefore, the missing values are imputed with K Nearest Neighbors (KNN) imputer \cite{scikit-learn_KNNImputer}. 
After imputation, a QuantileTransformer \cite{scikit-learn_QuantileTransformer} is applied to the dataset to normalize the features. 
The motivation for using QuantileTransformer is that it can transform the features to follow a uniform or normal distribution, 
which contributes to equal weight for distance-based clustering algorithms, and avoid the effect of some extreme outliers too.

After the quantile transformation with uniform output distribution, the features are reduced with Principal Component Analysis (PCA) \cite{MACKIEWICZ1993303} to reduce the dimensionality. 
This is useful to decorrelate the features and reduce the computatioanl complexity of the clustering algorithms. Here, the PCA algorithm will only keep the first several principal components to retain 75\% of the variance. 
Finally, a robust scaler \cite{scikit-learn_RobustScaler} is applied to the dataset to standardize the features.

\subsection{Clustering Algorithms}\label{subsec:clustering}
Three clusteirng algorithms are applied to the pre-processed dataset, including KMeans, DBSCAN and Agglomerative Clustering. 
With the setting of KMeans, the number of clusters is set to 8, which is the number of classes in the dataset. Considering the complexity of the dataset, we initial cluster centroids using sampling based on an empirical probability distribution of the points' contribution to the overall inertia. 
Moreover, fix number of times the k-means algorithm is run with different centroid seeds is set to 10.

For DBSCAN, to determine the optimal hyperparameters, We use the elbow method to find the optimal value of epsilon. Specifically, the K-distance graph is computed to find the elbow point with a proposed method \cite{Kdistance} in 2011. 
The minimum number of samples is empirically set to 5. Also, we set an additonal eps modifier to adjust epilson values if needed.

Regarding the setting of Agglomerative Clustering, the linkage is set to 'ward' to minimize the variance of the clusters being merged. Also, the number of clusters is set to 8.

\subsection{Evaluation}\label{subsec:evaluation}
The perfromance of the above-menioned clustering algorithms is evaluated with different metric \cite{scikit-learn_clustering}. 
With the given ground truth labels, homogeneous score and completeness score are evaluated to reflect the purity of the given cluster, and given class, respectively.
Moreover, the V-measure score is calculated to reflect the harmonic mean of homogeneity and completeness. 
Additionally, the adjusted rand index and adjusted mutual information are computed to reflect the similarity between the true labels and the predicted labels. 
Another common metric, silhouette score, is also utilized to reflect the compactness of the clusters. 
Lastly, Separation (SSB) and coherence (SSE) are specifically targeted for KMeans to evaluate the inter-cluster and intra-cluster sum of squares, respectively.



% Results section
\section{Results}\label{sec:results}
The metrics of three clustering algorithms are represented in table \ref{tab:clustering_results}.

The visualization of three clustering algorithms can be found in figure \ref{fig:clustering_visualization}. The subfigure \ref{fig:kmeans_visualization} shows the clustering result of KMeans, applied on a two-dimensional PCA-reduced dataset. 
The subfigure \ref{fig:dbscan_visualization} shows the clustering result of DBSCAN, and the subfigure \ref{fig:dendrogram_distribution} shows the dendrogram of Agglomerative Clustering.

The direct comparison of three clustering algorithms is shown in figure \ref{fig:comparison_clustering}.

\section{Conclusion}\label{sec:conclusion}
To conclude, three different clustering algorithms are applied to challenging Mice Protein Expression dataset. 
The KMeans and Agglomerative Clustering algorithms achieves similar relatively good perfromance, while DBSCAN fails to cluster the dataset accurately. 
It is also observed that DBSCAN is sensitive to the hyperparameters, and the elbow method is not always applicable to find the optimal hyperparameters.

% References section
\printbibliography


% Appendix section for all tables and figures
\section{Appendix}\label{sec:appendix}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.33\textwidth}
        \centering
        % set two figures height identical
        \includegraphics[width=\linewidth, height=4cm]{figures/kmeans_vis.png}
        \caption{KMeans clustering}
        \label{fig:kmeans_visualization}
    \end{subfigure}

    \hfill
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth, height=4cm]{figures/dbscan_vis.png}
        \caption{DBScan clustering}
        \label{fig:dbscan_visualization}
    \end{subfigure}
    
    \hfill
    \begin{subfigure}{0.33\textwidth}
        \centering
        % set two figures height identical
        \includegraphics[width=\linewidth, height=4cm]{figures/dendrogram_vis.png}
        \caption{Dendrogram of Agglomerative Clustering}
        \label{fig:dendrogram_distribution}
    \end{subfigure}
    \caption{Visualization of three clustering algorithms}
    \label{fig:clustering_visualization}
\end{figure}

% A table to show the clustering results on different metrics
\begin{table}[htbp]
    \centering
    \begin{tabular}{lccccccccc}
        \toprule
        \textbf{Algorithm} & \textbf{Time} & \textbf{Separation} & \textbf{Cohesion} & \textbf{Homogeneity} & \textbf{Completeness} & \textbf{V-measure} & \textbf{ARI} & \textbf{AMI} & \textbf{Silhouette Score} \\
        \midrule
        KMeans & 0.00 & 1.00 & 0.00 & 0.65 & 0.65 & 0.65 & 0.45 & 0.64 & 0.35 \\
        DBSCAN & 0.00 & 1.00 & 0.00 & 0.00 & 1.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
        Agglomerative & 0.00 & 1.00 & 0.00 & 0.65 & 0.65 & 0.65 & 0.45 & 0.64 & 0.35 \\
        \bottomrule
    \end{tabular}
    \caption{Clustering results on different metrics}
    \label{tab:clustering_results}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/clustering_result_comparison.png}
    \caption{The comparison of clustering algorithms (features are reduced to two dimensions for visualization purpose)}
    \label{fig:comparison_clustering}
\end{figure}

\end{document}